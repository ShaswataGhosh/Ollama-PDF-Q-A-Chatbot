

# ğŸ“š LLM-Powered PDF Q\&A App

This project is an interactive Question Answering (QA) application built using **LangChain**, **Ollama (LLaMA 3.2 model)**, and **Streamlit**. It enables users to ask natural language questions about the content of a PDF document and get context-aware answers.

## ğŸš€ Features

* ğŸ” Loads and processes PDF documents using `PyPDFLoader`
* ğŸ§  Uses Ollama's LLaMA 3.2 model for:

  * Embedding generation
  * Language understanding and response
* ğŸ”— Combines LangChain's prompt templates, retrievers, and output parsers
* ğŸ’¬ Real-time Q\&A through a Streamlit web interface

## ğŸ§° Tech Stack

* **Python**
* **LangChain**
* **Ollama (LLaMA 3.2)**
* **Streamlit**

## ğŸ› ï¸ How It Works

1. **Document Ingestion**: Loads a PDF (`attention.pdf`) and splits it into pages.
2. **Embeddings**: Generates vector embeddings using `OllamaEmbeddings`.
3. **Vector Store**: Stores embeddings in an in-memory vector database.
4. **Retrieval**: Retrieves the most relevant chunks of text based on the userâ€™s question.
5. **Prompting**: Uses a customizable prompt template to guide the language model.
6. **Answer Generation**: Produces and displays a final answer using the LLaMA model.

## ğŸ“„ Usage

To run the app:

```bash
streamlit run stream.py
```

## ğŸ“ File Structure

* `Project_01.py` â€“ Core logic for PDF processing, embedding, retrieval, and QA chain.
* `stream.py` â€“ Streamlit UI wrapper to run the app.
* `pyvenv.cfg` â€“ (Optional) Virtual environment configuration.

## âœ… Requirements

* `langchain`
* `streamlit`
* `ollama`
* `PyPDFLoader`
* `docarray`

Make sure Ollama and the LLaMA 3.2 model are properly installed and running.

---
